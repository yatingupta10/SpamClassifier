{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Summary:\n",
    "\n",
    "The last column of the spam dataset denotes whether the e-mail was considered spam (1) or not (0), i.e. unsolicited commercial e-mail.\n",
    "Most of the attributes indicate whether a particular word or character was frequently occuring in the e-mail.\n",
    "\n",
    "The data is downloaded from UCI Machine Learning Repository. For more information, refer to: https://archive.ics.uci.edu/ml/datasets/spambase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Decision Tree:\n",
    "\n",
    "A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value. It partitions the tree in recursively manner call recursive partitioning. This flowchart-like structure helps you in decision making. It's visualization like a flowchart diagram which easily mimics the human level thinking. That is why decision trees are easy to understand and interpret.\n",
    "\n",
    "### How does this work:\n",
    "The basic idea behind any decision tree algorithm is as follows:\n",
    "\n",
    "1. Select the best attribute using Attribute Selection Measures(ASM) to split the records.\n",
    "2. Make that attribute a decision node and breaks the dataset into smaller subsets.\n",
    "3. Starts tree building by repeating this process recursively for each child until one of the condition will match:\n",
    ">* All the tuples belong to the same attribute value.\n",
    ">* There are no more remaining attributes.\n",
    ">* There are no more instances.\n",
    "\n",
    "<img src=\"images/Fig1.png\">\n",
    "\n",
    "### Determine the best attribute test Condition\n",
    "The decision tree inducing algorithm must provide a method for specifying the test condition for different attribute types as well as an objective measure for evaluating the good ness of each test condition.\n",
    "\n",
    "First, the specification of an attribute test condition and its corresponding outcomes depends on the attribute types. We can do two-way split or multi-way split, discretize or group attribute values as needed. The binary attributes leads to two-way split test condition. For norminal attributes which have many values, the test condition can be expressed into multiway split on each distinct values, or two-way split by grouping the attribute values into two subsets. Similarly, the ordinal attributes can also produce binary or multiway splits as long as the grouping does not violate the order property of the attribute values. For continuous attributes, the test condition can be expressed as a comparsion test with two outcomes, or a range query. Or we can discretize the continous value into nominal attribute and then perform two-way or multi-way split.\n",
    "\n",
    "Since there are may choices to specify the test conditions from the given training set, we need use a measurement to determine the best way to split the records. The goal of best test conditions is whether it leads a homogenous class distribution in the nodes, which is the purity of the child nodes before and after spliting. The larger the degree of purity, the better the class distribution.\n",
    "\n",
    "To determine how well a test condition performs, we need to compare the degree of impurity of the parent before spliting with degree of the impurity of the child nodes after splitting. The larger their differnce, the better the test condition. The measurment of node impurity/purity are:\n",
    "* Gini Index\n",
    "* Entropy\n",
    "* Misclassification Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier #Decision Tree classifier\n",
    "from sklearn.model_selection import KFold #for K fold crossvalidation\n",
    "from sklearn.metrics import confusion_matrix #for confusion matrix\n",
    "from sklearn.metrics import accuracy_score # for accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data and renaming the last column as spam_class which denotes whether the e-mail was considered spam (1) or not (0), i.e. unsolicited commercial e-mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/spambase.data', header=None)\n",
    "data.rename(columns={57:'spam_class'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(data.pop('spam_class')) #y as output\n",
    "X = np.array(data) #X as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Fold Cross Validation:\n",
    "The general procedure is as follows:\n",
    "\n",
    "1. Shuffle the dataset randomly.\n",
    "2. Split the dataset into k groups\n",
    "3. For each unique group:\n",
    ">* Take the group as a hold out or test data set\n",
    ">* Take the remaining groups as a training data set\n",
    ">* Fit a model on the training set and evaluate it on the test set\n",
    ">* Retain the evaluation score and discard the model\n",
    "4. Summarize the skill of the model using the sample of model evaluation scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, random_state=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\t\t\n",
    "A decision tree is built top-down from a root node and involves partitioning the data into subsets that contain instances with similar values (homogenous). ID3 algorithm uses entropy to calculate the homogeneity of a sample. If the sample is completely homogeneous the entropy is zero and if the sample is an equally divided it has entropy of one.\n",
    "\n",
    "<img src=\"images/Fig2.png\">\n",
    "\n",
    "### Information Gain\t\t\n",
    "The information gain is based on the decrease in entropy after a dataset is split on an attribute. Constructing a decision tree is all about finding attribute that returns the highest information gain (i.e., the most homogeneous branches).\t\t\n",
    "Step 1: Calculate entropy of the target. \t\t\n",
    "Step 2: The dataset is then split on the different attributes. The entropy for each branch is calculated. Then it is added proportionally, to get total entropy for the split. The resulting entropy is subtracted from the entropy before the split. The result is the Information Gain, or decrease in entropy.\n",
    "<img src=\"images/Fig3.png\">\n",
    "Step 3: Choose attribute with the largest information gain as the decision node, divide the dataset by its branches and repeat the same process on every branch.\n",
    "Step 4a: A branch with entropy of 0 is a leaf node.\n",
    "Step 4b: A branch with entropy more than 0 needs further splitting.\t\t\n",
    "Step 5: The ID3 algorithm is run recursively on the non-leaf branches, until all data is classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_entropy = DecisionTreeClassifier(criterion = \"entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame()\n",
    "misclassified = 0 \n",
    "number_of_samples = 0\n",
    "fpr = 0.0\n",
    "fnr = 0.0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index] #Split data\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    clf_entropy = clf_entropy.fit(X_train,y_train)\n",
    "    \n",
    "    predictions = clf_entropy.predict(X_test) #calculating predictions\n",
    "    \n",
    "    accuracy = float(accuracy_score(y_test, predictions))\n",
    "    error_rate = float(1 - accuracy)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).flatten()\n",
    "    fpr = float(fp)/float(tp + tn + fn + fp)\n",
    "    fnr = float(fn)/float((tp + tn + fn + fp))\n",
    "    \n",
    "    number_of_samples += tp + tn + fn + fp\n",
    "    misclassified += fp + fn\n",
    "    \n",
    "    df2 = df2.append(pd.DataFrame({\"Error rate\": [error_rate],\"False positive rate\": [fpr],\"False negative rate \": [fnr]}),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Error rate  False negative rate   False positive rate\n",
      "0    0.060738              0.030369             0.030369\n",
      "1    0.078261              0.043478             0.034783\n",
      "2    0.115217              0.078261             0.036957\n",
      "3    0.091304              0.026087             0.065217\n",
      "4    0.082609              0.039130             0.043478\n",
      "5    0.065217              0.034783             0.030435\n",
      "6    0.078261              0.043478             0.034783\n",
      "7    0.054348              0.023913             0.030435\n",
      "8    0.065217              0.023913             0.041304\n",
      "9    0.071739              0.034783             0.036957\n",
      "Overall error rate:  0.0762877635297\n"
     ]
    }
   ],
   "source": [
    "print df2\n",
    "print 'Overall error rate: ', float(misclassified)/float(number_of_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gini Index:\n",
    "Gini index says, if we select two items from a population at random then they must be of same class and probability for this is 1 if population is pure.\n",
    "\n",
    "1. It works with categorical target variable “Success” or “Failure”.\n",
    "2. It performs only Binary splits\n",
    "3. Higher the value of Gini higher the homogeneity.\n",
    "4. CART (Classification and Regression Tree) uses Gini method to create binary splits.\n",
    "\n",
    "Steps to Calculate Gini for a split\n",
    "1. Calculate Gini for sub-nodes, using formula sum of square of probability for success and failure (p²+q²).\n",
    "2. Calculate Gini for split using weighted Gini score of each node of that split\n",
    "\n",
    "<img src=\"images/Fig4.png\">\n",
    "\n",
    "The Gini Index is calculated by subtracting the sum of the squared probabilities of each class from one.  It favors larger partitions.It considers a binary split for each attribute. You can compute a weighted sum of the impurity of each partition. If a binary split on attribute A partitions data D into D1 and D2, the Gini index of D is:\n",
    "<img src=\"images/Fig5.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gini = DecisionTreeClassifier(criterion = \"gini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame()\n",
    "misclassified = 0 \n",
    "number_of_samples = 0\n",
    "fpr = 0.0\n",
    "fnr = 0.0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index] #Split data\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    clf_gini = clf_gini.fit(X_train,y_train)\n",
    "    \n",
    "    predictions = clf_gini.predict(X_test) #calculating predictions\n",
    "    \n",
    "    accuracy = float(accuracy_score(y_test, predictions))\n",
    "    error_rate = float(1 - accuracy)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).flatten()\n",
    "    fpr = float(fp)/float(tp + tn + fn + fp)\n",
    "    fnr = float(fn)/float((tp + tn + fn + fp))\n",
    "    \n",
    "    number_of_samples += tp + tn + fn + fp\n",
    "    misclassified += fp + fn\n",
    "    \n",
    "    df3 = df3.append(pd.DataFrame({\"Error rate\": [error_rate],\"False positive rate\": [fpr],\"False negative rate \": [fnr]}),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Error rate  False negative rate   False positive rate\n",
      "0    0.071584              0.039046             0.032538\n",
      "1    0.082609              0.023913             0.058696\n",
      "2    0.073913              0.043478             0.030435\n",
      "3    0.089130              0.034783             0.054348\n",
      "4    0.089130              0.032609             0.056522\n",
      "5    0.076087              0.036957             0.039130\n",
      "6    0.089130              0.045652             0.043478\n",
      "7    0.100000              0.065217             0.034783\n",
      "8    0.089130              0.030435             0.058696\n",
      "9    0.086957              0.036957             0.050000\n",
      "Overall error rate:  0.0847641816996\n"
     ]
    }
   ],
   "source": [
    "print df3\n",
    "print 'Overall error rate: ', float(misclassified)/float(number_of_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
